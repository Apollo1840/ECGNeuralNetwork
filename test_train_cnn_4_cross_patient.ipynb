{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/congyu/congyu_program/pythons/forks/ECGNeuralNetwork\n"
     ]
    }
   ],
   "source": [
    "from cnn.cnn import load_dataset, create_model, steps\n",
    "from dataset import dataset, load_files\n",
    "from utilities.labels import LABELS\n",
    "from dataset.data_augmentation import augmentated_filenames2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/mit_bih/\"\n",
    "save_dir = \"./data/beats_img/\"\n",
    "_dataset_dir = './data/beats_img'\n",
    "_model = './trained_models/cnn_aami_big_compare_cross.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = None\n",
    "validation = None\n",
    "augmentation = False\n",
    "_batch_size = 32\n",
    "_size = (64, 64)\n",
    "_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load DS tvt from ./data/beats_img\n",
      "71637\n",
      "17910\n",
      "86109\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = dataset.load_files_cross_patient(_dataset_dir, \n",
    "                                             verbose=True, \n",
    "                                             keep_ratio=1)\n",
    "    \n",
    "print(len(train))\n",
    "print(len(validation))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = create_model(out_dim=4)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n",
      "2239/2238 [==============================] - 223s 99ms/step - loss: 0.1867 - accuracy: 0.9683 - val_loss: 0.4530 - val_accuracy: 0.9455\n",
      "Epoch 2/50\n",
      "2239/2238 [==============================] - 214s 96ms/step - loss: 0.1095 - accuracy: 0.9830 - val_loss: 0.6704 - val_accuracy: 0.9461\n",
      "Epoch 3/50\n",
      "2239/2238 [==============================] - 190s 85ms/step - loss: 0.0954 - accuracy: 0.9862 - val_loss: 0.0899 - val_accuracy: 0.9461\n",
      "Epoch 4/50\n",
      "2239/2238 [==============================] - 198s 89ms/step - loss: 0.0876 - accuracy: 0.9875 - val_loss: 0.0571 - val_accuracy: 0.9787\n",
      "Epoch 5/50\n",
      "2239/2238 [==============================] - 195s 87ms/step - loss: 0.0839 - accuracy: 0.9885 - val_loss: 0.1431 - val_accuracy: 0.9461\n",
      "Epoch 6/50\n",
      "2239/2238 [==============================] - 298s 133ms/step - loss: 0.0826 - accuracy: 0.9885 - val_loss: 0.0664 - val_accuracy: 0.9816\n",
      "Epoch 7/50\n",
      "2239/2238 [==============================] - 277s 124ms/step - loss: 0.0787 - accuracy: 0.9897 - val_loss: 0.0834 - val_accuracy: 0.9743\n",
      "Epoch 8/50\n",
      "2239/2238 [==============================] - 190s 85ms/step - loss: 0.0722 - accuracy: 0.9905 - val_loss: 0.0530 - val_accuracy: 0.9778\n",
      "Epoch 9/50\n",
      "2239/2238 [==============================] - 185s 83ms/step - loss: 0.0682 - accuracy: 0.9908 - val_loss: 0.0415 - val_accuracy: 0.9705\n",
      "Epoch 10/50\n",
      "2239/2238 [==============================] - 188s 84ms/step - loss: 0.0652 - accuracy: 0.9916 - val_loss: 0.0923 - val_accuracy: 0.9704\n",
      "Epoch 11/50\n",
      "2239/2238 [==============================] - 196s 87ms/step - loss: 0.0651 - accuracy: 0.9914 - val_loss: 0.3081 - val_accuracy: 0.7831\n",
      "Epoch 12/50\n",
      "2239/2238 [==============================] - 190s 85ms/step - loss: 0.0650 - accuracy: 0.9916 - val_loss: 0.0373 - val_accuracy: 0.9757\n",
      "Epoch 13/50\n",
      "2239/2238 [==============================] - 183s 82ms/step - loss: 0.0639 - accuracy: 0.9917 - val_loss: 0.0441 - val_accuracy: 0.9743\n",
      "Epoch 14/50\n",
      "2239/2238 [==============================] - 197s 88ms/step - loss: 0.0636 - accuracy: 0.9919 - val_loss: 0.0677 - val_accuracy: 0.9676\n",
      "Epoch 15/50\n",
      "2239/2238 [==============================] - 185s 83ms/step - loss: 0.0618 - accuracy: 0.9921 - val_loss: 0.1640 - val_accuracy: 0.9677\n",
      "Epoch 16/50\n",
      "2239/2238 [==============================] - 183s 82ms/step - loss: 0.0579 - accuracy: 0.9925 - val_loss: 0.0674 - val_accuracy: 0.9780\n",
      "Epoch 17/50\n",
      "2239/2238 [==============================] - 175s 78ms/step - loss: 0.0555 - accuracy: 0.9929 - val_loss: 0.1004 - val_accuracy: 0.9759\n",
      "Epoch 18/50\n",
      "2239/2238 [==============================] - 170s 76ms/step - loss: 0.0532 - accuracy: 0.9932 - val_loss: 0.0569 - val_accuracy: 0.9758\n",
      "Epoch 19/50\n",
      "2239/2238 [==============================] - 170s 76ms/step - loss: 0.0490 - accuracy: 0.9942 - val_loss: 0.0452 - val_accuracy: 0.9792\n",
      "Epoch 20/50\n",
      "2239/2238 [==============================] - 170s 76ms/step - loss: 0.0478 - accuracy: 0.9942 - val_loss: 0.3034 - val_accuracy: 0.9678\n",
      "Epoch 21/50\n",
      "2239/2238 [==============================] - 170s 76ms/step - loss: 0.0458 - accuracy: 0.9946 - val_loss: 0.1410 - val_accuracy: 0.9623\n",
      "Epoch 22/50\n",
      "2239/2238 [==============================] - 170s 76ms/step - loss: 0.0425 - accuracy: 0.9951 - val_loss: 0.1082 - val_accuracy: 0.9608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f79e1b85d68>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "if augmentation:\n",
    "    train = augmentated_filenames2(train)\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(_model, monitor='val_loss', save_best_only=False),\n",
    "                  EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto',\n",
    "                                baseline=None, restore_best_weights=False),\n",
    "                  ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, verbose=0, mode='auto')\n",
    "# TrainValTensorBoard(write_graph=False)\n",
    "]\n",
    "\n",
    "data_gen_train = load_dataset(train, _dataset_dir,\n",
    "                              _batch_size, _size,\n",
    "                              label_type = \"AAMI\",\n",
    "                              random_crop=augmentation,\n",
    "                              random_rotate=augmentation,\n",
    "                              flip=augmentation)\n",
    "\n",
    "data_gen_valid = load_dataset(validation, _dataset_dir,\n",
    "                              _batch_size, _size,\n",
    "                              label_type = \"AAMI\",\n",
    "                              random_crop=augmentation,\n",
    "                              random_rotate=augmentation,\n",
    "                              flip=augmentation)\n",
    "\n",
    "model.fit_generator(data_gen_train,\n",
    "                    steps_per_epoch=steps(train, _batch_size),\n",
    "                    epochs=_epochs,\n",
    "                    validation_data=data_gen_valid,\n",
    "                    validation_steps=steps(validation, _batch_size),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluate_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing 86109 images to arrays\n",
      "predicting on 86109 samples\n",
      "precision:  0.9340131613569144\n",
      "recall:  0.9285556678163722\n",
      "f1score:  0.9300953193120629\n",
      "macro f1 score 0.4611162607270816\n",
      "micro f1 score 0.9285556678163722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.04      0.09      0.05       388\n",
      "           N       0.97      0.96      0.96     80451\n",
      "        SVEB       0.13      0.08      0.10      2049\n",
      "         VEB       0.62      0.89      0.73      3221\n",
      "\n",
      "    accuracy                           0.93     86109\n",
      "   macro avg       0.44      0.51      0.46     86109\n",
      "weighted avg       0.93      0.93      0.93     86109\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = evaluate_cnn(model, label_type=\"AAMI\", cross_patient=True, keep_ratio=1)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score 0.4611162607270816\n",
      "micro f1 score 0.9285556678163722\n"
     ]
    }
   ],
   "source": [
    "print(\"macro f1 score\", f1_score(y_true, y_pred, average='macro'))\n",
    "print(\"micro f1 score\", f1_score(y_true, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "congyuml",
   "language": "python",
   "name": "congyuml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
