{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.cnn import load_dataset, create_model, steps\n",
    "from dataset import dataset, load_files\n",
    "from utilities.labels import LABELS\n",
    "from dataset.data_augmentation import augmentated_filenames2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/mit_bih/\"\n",
    "save_dir = \"./data/beats_img/\"\n",
    "_dataset_dir = './data/beats_img'\n",
    "_model = './trained_models/cnn_baseline.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = None\n",
    "validation = None\n",
    "augmentation = False\n",
    "_batch_size = 32\n",
    "_size = (64, 64)\n",
    "_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "load tvt from ./data/beats_img\n",
      "7533\n",
      "1614\n",
      "1614\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "train, validation, test = dataset.load_files(_dataset_dir, \n",
    "                                             verbose=True, \n",
    "                                             keep_ratio=0.1)\n",
    "    \n",
    "print(len(train))\n",
    "print(len(validation))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/congyu/.virtualenvs/congyuml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/30\n",
      "236/235 [==============================] - 20s 84ms/step - loss: 1.1105 - accuracy: 0.7540 - val_loss: 1.6374 - val_accuracy: 0.6797\n",
      "Epoch 2/30\n",
      "236/235 [==============================] - 18s 78ms/step - loss: 0.5025 - accuracy: 0.8889 - val_loss: 1.3310 - val_accuracy: 0.8631\n",
      "Epoch 3/30\n",
      "236/235 [==============================] - 18s 75ms/step - loss: 0.3532 - accuracy: 0.9327 - val_loss: 0.9509 - val_accuracy: 0.8612\n",
      "Epoch 4/30\n",
      "236/235 [==============================] - 18s 78ms/step - loss: 0.2940 - accuracy: 0.9498 - val_loss: 0.6912 - val_accuracy: 0.9244\n",
      "Epoch 5/30\n",
      "236/235 [==============================] - 18s 77ms/step - loss: 0.2602 - accuracy: 0.9559 - val_loss: 1.1694 - val_accuracy: 0.7937\n",
      "Epoch 6/30\n",
      "236/235 [==============================] - 18s 77ms/step - loss: 0.2417 - accuracy: 0.9618 - val_loss: 1.1424 - val_accuracy: 0.7869\n",
      "Epoch 7/30\n",
      "236/235 [==============================] - 18s 77ms/step - loss: 0.2136 - accuracy: 0.9701 - val_loss: 0.3280 - val_accuracy: 0.9442\n",
      "Epoch 8/30\n",
      "236/235 [==============================] - 18s 77ms/step - loss: 0.2082 - accuracy: 0.9725 - val_loss: 1.0129 - val_accuracy: 0.8302\n",
      "Epoch 9/30\n",
      "236/235 [==============================] - 18s 77ms/step - loss: 0.2038 - accuracy: 0.9728 - val_loss: 1.2099 - val_accuracy: 0.8587\n",
      "Epoch 10/30\n",
      "236/235 [==============================] - 18s 77ms/step - loss: 0.1883 - accuracy: 0.9802 - val_loss: 0.7549 - val_accuracy: 0.8817\n",
      "Epoch 11/30\n",
      "236/235 [==============================] - 18s 76ms/step - loss: 0.1692 - accuracy: 0.9862 - val_loss: 0.3683 - val_accuracy: 0.9480\n",
      "Epoch 12/30\n",
      "236/235 [==============================] - 18s 77ms/step - loss: 0.1624 - accuracy: 0.9853 - val_loss: 0.5049 - val_accuracy: 0.9257\n",
      "Epoch 13/30\n",
      "236/235 [==============================] - 18s 77ms/step - loss: 0.1537 - accuracy: 0.9879 - val_loss: 0.2313 - val_accuracy: 0.9591\n",
      "Epoch 14/30\n",
      "236/235 [==============================] - 18s 77ms/step - loss: 0.1607 - accuracy: 0.9851 - val_loss: 1.0030 - val_accuracy: 0.7943\n",
      "Epoch 15/30\n",
      "236/235 [==============================] - 18s 77ms/step - loss: 0.1581 - accuracy: 0.9865 - val_loss: 0.2580 - val_accuracy: 0.9176\n",
      "Epoch 16/30\n",
      "236/235 [==============================] - 19s 79ms/step - loss: 0.1453 - accuracy: 0.9879 - val_loss: 0.1920 - val_accuracy: 0.9529\n",
      "Epoch 17/30\n",
      "236/235 [==============================] - 18s 74ms/step - loss: 0.1511 - accuracy: 0.9867 - val_loss: 0.1762 - val_accuracy: 0.9504\n",
      "Epoch 18/30\n",
      "236/235 [==============================] - 17s 74ms/step - loss: 0.1383 - accuracy: 0.9916 - val_loss: 0.4531 - val_accuracy: 0.9120\n",
      "Epoch 19/30\n",
      "236/235 [==============================] - 18s 74ms/step - loss: 0.1406 - accuracy: 0.9885 - val_loss: 0.6147 - val_accuracy: 0.9257\n",
      "Epoch 20/30\n",
      "236/235 [==============================] - 18s 74ms/step - loss: 0.1427 - accuracy: 0.9879 - val_loss: 0.7251 - val_accuracy: 0.8804\n",
      "Epoch 21/30\n",
      "236/235 [==============================] - 17s 74ms/step - loss: 0.1257 - accuracy: 0.9939 - val_loss: 0.1023 - val_accuracy: 0.9734\n",
      "Epoch 22/30\n",
      "236/235 [==============================] - 17s 74ms/step - loss: 0.1119 - accuracy: 0.9951 - val_loss: 0.1299 - val_accuracy: 0.9734\n",
      "Epoch 23/30\n",
      "236/235 [==============================] - 17s 74ms/step - loss: 0.1169 - accuracy: 0.9932 - val_loss: 0.8711 - val_accuracy: 0.8829\n",
      "Epoch 24/30\n",
      "236/235 [==============================] - 18s 78ms/step - loss: 0.1146 - accuracy: 0.9932 - val_loss: 0.1811 - val_accuracy: 0.9449\n",
      "Epoch 25/30\n",
      "236/235 [==============================] - 26s 112ms/step - loss: 0.0996 - accuracy: 0.9969 - val_loss: 0.0878 - val_accuracy: 0.9758\n",
      "Epoch 26/30\n",
      "236/235 [==============================] - 25s 105ms/step - loss: 0.1022 - accuracy: 0.9944 - val_loss: 0.0897 - val_accuracy: 0.9647\n"
     ]
    }
   ],
   "source": [
    "if augmentation:\n",
    "    train = augmentated_filenames2(train)\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(_model, monitor='val_loss', save_best_only=False),\n",
    "                  EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto',\n",
    "                                baseline=None, restore_best_weights=False),\n",
    "                  ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, verbose=0, mode='auto')\n",
    "# TrainValTensorBoard(write_graph=False)\n",
    "]\n",
    "\n",
    "data_gen_train = load_dataset(train, _dataset_dir,\n",
    "                              _batch_size, _size,\n",
    "                              random_crop=augmentation,\n",
    "                              random_rotate=augmentation,\n",
    "                              flip=augmentation)\n",
    "\n",
    "data_gen_valid = load_dataset(validation, _dataset_dir,\n",
    "                              _batch_size, _size,\n",
    "                              random_crop=augmentation,\n",
    "                              random_rotate=augmentation,\n",
    "                              flip=augmentation)\n",
    "\n",
    "model.fit_generator(data_gen_train,\n",
    "                    steps_per_epoch=steps(train, _batch_size),\n",
    "                    epochs=_epochs,\n",
    "                    validation_data=data_gen_valid,\n",
    "                    validation_steps=steps(validation, _batch_size),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluate_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_cnn(model, keep_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "congyuml",
   "language": "python",
   "name": "congyuml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
